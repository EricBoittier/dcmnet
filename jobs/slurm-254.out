gpu02
no change     /pchem-data/meuwly/boittier/home/miniforge3/condabin/conda
no change     /pchem-data/meuwly/boittier/home/miniforge3/bin/conda
no change     /pchem-data/meuwly/boittier/home/miniforge3/bin/conda-env
no change     /pchem-data/meuwly/boittier/home/miniforge3/bin/activate
no change     /pchem-data/meuwly/boittier/home/miniforge3/bin/deactivate
no change     /pchem-data/meuwly/boittier/home/miniforge3/etc/profile.d/conda.sh
no change     /pchem-data/meuwly/boittier/home/miniforge3/etc/fish/conf.d/conda.fish
no change     /pchem-data/meuwly/boittier/home/miniforge3/shell/condabin/Conda.psm1
no change     /pchem-data/meuwly/boittier/home/miniforge3/shell/condabin/conda-hook.ps1
no change     /pchem-data/meuwly/boittier/home/miniforge3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /pchem-data/meuwly/boittier/home/miniforge3/etc/profile.d/conda.csh
no change     /pchem-data/meuwly/boittier/home/.bashrc
No action taken.
/pchem-data/meuwly/boittier/home/miniforge3/envs/jaxe3xcuda11p39/bin/python
GPU ID: 0
restart /pchem-data/meuwly/boittier/home/jaxeq/all_runs/runs11/20240924-141337dcm-2-espw-1000.0-restart-False/best_1000.0_params.pkl
2024-09-25 12:47:31.088700: W external/xla/xla/service/gpu/gemm_fusion_autotuner.cc:807] Compiling 15 configs for gemm_fusion_dot.2 on a single thread.
2024-09-25 12:47:34.890621: W external/xla/xla/service/gpu/gemm_fusion_autotuner.cc:807] Compiling 25 configs for gemm_fusion_dot.2 on a single thread.
2024-09-25 12:47:45.207805: W external/xla/xla/service/gpu/gemm_fusion_autotuner.cc:807] Compiling 19 configs for gemm_fusion_dot.2 on a single thread.
args:
data_dir = /pchem-data/meuwly/boittier/home/jaxeq/
model_dir = model
num_epochs = 5000
learning_rate = 0.0001
batch_size = 1
esp_w = 1000.0
num_epics = 1
n_feat = 16
n_basis = 16
max_degree = 2
n_mp = 2
restart = /pchem-data/meuwly/boittier/home/jaxeq/all_runs/runs11/20240924-141337dcm-2-espw-1000.0-restart-False/best_1000.0_params.pkl
random_seed = 15828
n_dcm = 1
n_gpu = 0
data = qm9-esp40000-0.npz
n_train = 64000
n_valid = 2000
type = dipole
include_pseudotensors = False
[cuda(id=0)]
gpu
[cuda(id=0)]
['/pchem-data/meuwly/boittier/home/jaxeq/dcmnet', '/pchem-data/meuwly/boittier/home/miniforge3/envs/jaxe3xcuda11p39/lib/python39.zip', '/pchem-data/meuwly/boittier/home/miniforge3/envs/jaxe3xcuda11p39/lib/python3.9', '/pchem-data/meuwly/boittier/home/miniforge3/envs/jaxe3xcuda11p39/lib/python3.9/lib-dynload', '/pchem-data/meuwly/boittier/home/miniforge3/envs/jaxe3xcuda11p39/lib/python3.9/site-packages', '/pchem-data/meuwly/boittier/home/jaxeq']
{'features': 16.0, 'max_degree': 2.0, 'num_iterations': 2.0, 'num_basis_functions': 16.0, 'cutoff': 4.0, 'n_dcm': 2.0, 'include_pseudotensors': True}
R (40000, 60, 3)
Z (40000, 60)
N (40000,)
D (40000,)
com (40000, 3)
Dxyz (40000, 3)
mono (40000, 60, 1)
esp (40000, 3200)
id (40000,)
n_grid (40000,)
vdw_surface (40000, 3200, 3)
R (40000, 60, 3)
Z (40000, 60)
N (40000,)
D (40000,)
com (40000, 3)
Dxyz (40000, 3)
mono (40000, 60, 1)
esp (40000, 3200)
id (40000,)
n_grid (40000,)
vdw_surface (40000, 3200, 3)
R (40000, 60, 3)
Z (40000, 60)
N (40000,)
D (40000,)
com (40000, 3)
Dxyz (40000, 3)
mono (40000, 60, 1)
esp (40000, 3200)
id (40000,)
n_grid (40000,)
vdw_surface (40000, 3200, 3)
n_failed: 1425
62575 2000
creating_mask
118575
/pchem-data/meuwly/boittier/home/jaxeq/all_runs/runs11/20240925-124719dcm-1-espw-1000.0-restart-True
epic 1
Preparing batches
..................
jax.errors.SimplifiedTraceback: For simplicity, JAX has removed its internal frames from the traceback of the following exception. Set JAX_TRACEBACK_FILTERING=off to include these.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/pchem-data/meuwly/boittier/home/jaxeq/dcmnet/main.py", line 124, in <module>
    params, val = training(
  File "/pchem-data/meuwly/boittier/home/jaxeq/dcmnet/training_dipole.py", line 124, in train_model_dipo
    params, opt_state, loss = train_step_dipo(
  File "/pchem-data/meuwly/boittier/home/jaxeq/dcmnet/training_dipole.py", line 44, in train_step_dipo
    (loss, (mono, dipo)), grad = jax.value_and_grad(loss_fn, has_aux=True)(params)
  File "/pchem-data/meuwly/boittier/home/jaxeq/dcmnet/training_dipole.py", line 28, in loss_fn
    loss = dipo_esp_mono_loss(
  File "/pchem-data/meuwly/boittier/home/jaxeq/dcmnet/loss.py", line 90, in dipo_esp_mono_loss
    d = jnp.moveaxis(dipo_prediction, -1, -2).reshape(batch_size, NATOMS * n_dcm, 3)
  File "/pchem-data/meuwly/boittier/home/miniforge3/envs/jaxe3xcuda11p39/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py", line 745, in meth
    return getattr(self.aval, name).fun(self, *args, **kwargs)
  File "/pchem-data/meuwly/boittier/home/miniforge3/envs/jaxe3xcuda11p39/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py", line 149, in _reshape
    newshape = _compute_newshape(a, args[0] if len(args) == 1 else args)
  File "/pchem-data/meuwly/boittier/home/miniforge3/envs/jaxe3xcuda11p39/lib/python3.9/site-packages/jax/_src/numpy/array_methods.py", line 137, in _compute_newshape
    raise TypeError(f"cannot reshape array of shape {np.shape(a)} (size {np.size(a)}) "
TypeError: cannot reshape array of shape (60, 2, 3) (size 360) into shape (1, 60, 3) (size 180)
